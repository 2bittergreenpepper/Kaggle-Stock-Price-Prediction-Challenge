"""
For Kaggle Stock Price Prediction Challenge 
by 2bitterGreenPepper
2025-09-03

Machine specification for reproducibility
Laptop:     HP ZHAN G6
Processer:  13th Gen Intel(R) Core(TM) i5-1340P (1.90 GHz)
RAM:        16.0 GB
System:     64-bit, x64
OS:         Win11, 24H2, 261000.4946
Python:     3.12.3
"""

import numpy as np
import os
from copy import copy
import pandas as pd
import random
import torch
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

# load data
def load_data(data_path):
    """ and create important features"""
    stock_names = os.listdir(data_path)
    X_stacks = []
    y_stacks = []
    
    for jt in range(len(stock_names)):
        # load each stock
        df = pd.read_csv(data_path+stock_names[jt],index_col=0)
        
        # basic features
        df['Vlog'] = np.log10(df['Volume'].values)
        price_keys = ['Open', 'High', 'Low', 'Close', 'Adjusted', 'Vlog']
        X_keys = price_keys+[]
        y_keys = []

        # my MAs
        for key in price_keys:
            X_keys.append(key+'_fl')
            df[X_keys[-1]]=df[key].rolling(6).mean()
        
        # you always need yesterday's candlesticks
        for it in range(1,3):
            for key in price_keys:
                X_keys.append(key+'_%d'%(it))
                df[X_keys[-1]] = df[key].shift(it)
        
        # the y data
        for it in range(1,12):
            y_keys.append('Returns_%d'%(it))
            df[y_keys[-1]] = df['Returns'].shift(-it)
        
        # drop nan
        df.dropna(inplace=True)
    
        X = np.array(df[X_keys].values)
        y = np.array(df[y_keys].values)
        X_stacks.append(X)
        y_stacks.append(y)

    return np.vstack(X_stacks), np.vstack(y_stacks)

class CustomImageDataset(Dataset):
    def __init__(self, X, y):
        self.y = torch.FloatTensor(y)
        self.X = torch.FloatTensor(X)
    def __len__(self):
        return len(self.y)
    def __getitem__(self, idx):
        image = self.X[idx]
        label = self.y[idx]
        return image, label

# preditive model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(24, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 11),
        )
    def forward(self, x):
        return self.linear_relu_stack(x)

# training loop
def train_loop(dataloader, model, loss_fn, optimizer, batch_size, verbose=True):
    size = len(dataloader.dataset)

    # Set the model to training mode
    # Unnecessary in this situation but added for best practices
    model.train()
    loss_ave = 0

    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

		# batch counter, starting from 0
        if batch % 100 == 0:
            loss, current = loss.item(), batch * batch_size + len(X)
            if verbose:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
            loss_ave += loss
    
    loss_ave /=(batch+1)*100
    return loss_ave

# test loop
def test_loop(dataloader, model, loss_fn, verbose=True):
    size = len(dataloader.dataset)

    # Set the model to evaluation mode
    # Unnecessary in this situation but added for best practices
    model.eval()

    num_batches = len(dataloader) # when defining dataloader, we specified the batch size, the last batch may be less.

    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
    
    test_loss /= num_batches

    if verbose:
        print(f"Test Error: {test_loss:>8f} \n")

    return test_loss


# make submission file
def make_submission(file_path, model):
    stock_names = os.listdir(file_path+'test/')

    Y = []
    # all five tests
    for jt in range(5):
        df = pd.read_csv(file_path+'test/'+stock_names[jt],index_col=0)
        df['Vlog'] = np.log10(df['Volume'].values)
        price_keys = ['Open', 'High', 'Low', 'Close', 'Adjusted', 'Vlog']
        X_keys = price_keys+[]
        y_keys = []

        for key in price_keys:
            X_keys.append(key+'_fl')
            df[X_keys[-1]]=df[key].rolling(6).mean()

        for it in range(1,3):
            for key in price_keys:
                X_keys.append(key+'_%d'%(it))
                df[X_keys[-1]] = df[key].shift(it)

        for it in range(1,12):
            y_keys.append('Returns_%d'%(it))
            df[y_keys[-1]] = df['Returns'].shift(-it)

        # extract x for prediction
        x = torch.FloatTensor(df.iloc[-1][X_keys].values.astype(float)).unsqueeze(0)

        with torch.no_grad():
            y_pred = model(x)

        Y.append(y_pred.squeeze(0))

    return np.array(Y).T

# Run the pipeline
if __name__ == "__main__":
    # seeding for reproducibility
    seed = 0
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # load data
    file_path = '/kaggle/input/stock-price-prediction-challenge/'
    print('loading data')

    X_train, y_train = load_data(file_path+'train/stocks/')
    X_test, y_test = load_data(file_path+'test/')

    train_data = CustomImageDataset(X_train, y_train)
    test_data = CustomImageDataset(X_test, y_test)

    train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
  
    # training parameters   
    learning_rate = 3e-4
    batch_size = 64
    epochs = 80

    model = NeuralNetwork()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    loss_fn = nn.MSELoss()

    # log losses
    train_loss = []
    test_loss = []

    # start training
    print('begin training')
    for t in range(epochs):
        print(f"Epoch {t+1}\n-------------------------------")
        v = train_loop(train_dataloader, model, loss_fn, optimizer, batch_size)
        v2 = test_loop(test_dataloader, model, loss_fn)
        # log losses
        train_loss.append(v)
        test_loss.append(v2)

    # make predictions (of the next 11 days)
    Y = make_submission(file_path,model)

    # submission file
    file = pd.read_csv(file_path+'sample_submission.csv',index_col=0)
    for it in range(1,6):
        file['Returns_%d'%(it)] = Y[1:,it-1]
    
    copy(file).to_csv('my_submission.csv')
    print(file) # oops can't save file
    
    # visualize training
    plt.semilogy(train_loss,'o-')
    plt.plot(test_loss,'^-')
    plt.show()

"""
Example Output

loading data
begin training
Epoch 1
-------------------------------
loss: 148.627182  [   64/114566]
loss: 0.001420  [ 6464/114566]
loss: 0.001392  [12864/114566]
loss: 0.002301  [19264/114566]
loss: 0.001150  [25664/114566]
loss: 0.001599  [32064/114566]
loss: 0.001332  [38464/114566]
loss: 0.001233  [44864/114566]
loss: 0.001399  [51264/114566]
loss: 0.001507  [57664/114566]
loss: 0.001181  [64064/114566]
loss: 0.001194  [70464/114566]
loss: 0.001454  [76864/114566]
loss: 0.001126  [83264/114566]
loss: 0.001032  [89664/114566]
loss: 0.001231  [96064/114566]
loss: 0.001339  [102464/114566]
loss: 0.001390  [108864/114566]
Test Error: 0.001702 

Epoch 2
-------------------------------
loss: 0.001762  [   64/114566]
loss: 0.001103  [ 6464/114566]
loss: 0.001179  [12864/114566]
loss: 0.001018  [19264/114566]
loss: 0.001365  [25664/114566]
loss: 0.001746  [32064/114566]
loss: 0.001245  [38464/114566]
loss: 0.001253  [44864/114566]
loss: 0.001045  [51264/114566]
loss: 0.001135  [57664/114566]
loss: 0.000980  [64064/114566]
loss: 0.001218  [70464/114566]
loss: 0.000930  [76864/114566]
loss: 0.001218  [83264/114566]
loss: 0.000957  [89664/114566]
loss: 0.001196  [96064/114566]
loss: 0.000936  [102464/114566]
loss: 0.001120  [108864/114566]
Test Error: 0.001466 

Epoch 3
-------------------------------
loss: 0.001078  [   64/114566]
loss: 0.000803  [ 6464/114566]
loss: 0.001068  [12864/114566]
loss: 0.000924  [19264/114566]
loss: 0.001219  [25664/114566]
loss: 0.001335  [32064/114566]
loss: 0.001493  [38464/114566]
loss: 0.001983  [44864/114566]
loss: 0.000941  [51264/114566]
loss: 0.000933  [57664/114566]
loss: 0.001066  [64064/114566]
loss: 0.000842  [70464/114566]
loss: 0.000986  [76864/114566]
loss: 0.000830  [83264/114566]
loss: 0.000896  [89664/114566]
loss: 0.001035  [96064/114566]
loss: 0.000896  [102464/114566]
loss: 0.000889  [108864/114566]
Test Error: 0.001332 

Epoch 4
-------------------------------
loss: 0.001337  [   64/114566]
loss: 0.000832  [ 6464/114566]
loss: 0.000829  [12864/114566]
loss: 0.000858  [19264/114566]
loss: 0.000808  [25664/114566]
loss: 0.000646  [32064/114566]
loss: 0.000899  [38464/114566]
loss: 0.000738  [44864/114566]
loss: 0.000800  [51264/114566]
loss: 0.000862  [57664/114566]
loss: 0.001102  [64064/114566]
loss: 0.000828  [70464/114566]
loss: 0.000688  [76864/114566]
loss: 0.000770  [83264/114566]
loss: 0.000725  [89664/114566]
loss: 0.000854  [96064/114566]
loss: 0.000849  [102464/114566]
loss: 0.000749  [108864/114566]
Test Error: 0.001229 

Epoch 5
-------------------------------
loss: 0.000796  [   64/114566]
loss: 0.001163  [ 6464/114566]
loss: 0.000962  [12864/114566]
loss: 0.001036  [19264/114566]
loss: 0.001026  [25664/114566]
loss: 0.000828  [32064/114566]
loss: 0.000720  [38464/114566]
loss: 0.000675  [44864/114566]
loss: 0.000715  [51264/114566]
loss: 0.000709  [57664/114566]
loss: 0.000651  [64064/114566]
loss: 0.000705  [70464/114566]
loss: 0.000863  [76864/114566]
loss: 0.000947  [83264/114566]
loss: 0.000713  [89664/114566]
loss: 0.000981  [96064/114566]
loss: 0.000599  [102464/114566]
loss: 0.001050  [108864/114566]
Test Error: 0.001144 

Epoch 6
-------------------------------
loss: 0.000645  [   64/114566]
loss: 0.001132  [ 6464/114566]
loss: 0.000672  [12864/114566]
loss: 0.000652  [19264/114566]
loss: 0.000918  [25664/114566]
loss: 0.000896  [32064/114566]
loss: 0.001037  [38464/114566]
loss: 0.000911  [44864/114566]
loss: 0.000667  [51264/114566]
loss: 0.000932  [57664/114566]
loss: 0.000671  [64064/114566]
loss: 0.000773  [70464/114566]
loss: 0.000653  [76864/114566]
loss: 0.000760  [83264/114566]
loss: 0.000805  [89664/114566]
loss: 0.000856  [96064/114566]
loss: 0.000646  [102464/114566]
loss: 0.000931  [108864/114566]
Test Error: 0.001081 

Epoch 7
-------------------------------
loss: 0.000606  [   64/114566]
loss: 0.000794  [ 6464/114566]
loss: 0.000598  [12864/114566]
loss: 0.000617  [19264/114566]
loss: 0.000708  [25664/114566]
loss: 0.001235  [32064/114566]
loss: 0.001054  [38464/114566]
loss: 0.000807  [44864/114566]
loss: 0.002054  [51264/114566]
loss: 0.000715  [57664/114566]
loss: 0.000642  [64064/114566]
loss: 0.000761  [70464/114566]
loss: 0.000560  [76864/114566]
loss: 0.000806  [83264/114566]
loss: 0.000684  [89664/114566]
loss: 0.000757  [96064/114566]
loss: 0.000626  [102464/114566]
loss: 0.000593  [108864/114566]
Test Error: 0.001035 

Epoch 8
-------------------------------
loss: 0.000607  [   64/114566]
loss: 0.000624  [ 6464/114566]
loss: 0.000741  [12864/114566]
loss: 0.000489  [19264/114566]
loss: 0.000742  [25664/114566]
loss: 0.001250  [32064/114566]
loss: 0.000436  [38464/114566]
loss: 0.000607  [44864/114566]
loss: 0.000843  [51264/114566]
loss: 0.000505  [57664/114566]
loss: 0.000528  [64064/114566]
loss: 0.000685  [70464/114566]
loss: 0.000702  [76864/114566]
loss: 0.000666  [83264/114566]
loss: 0.000743  [89664/114566]
loss: 0.000610  [96064/114566]
loss: 0.000751  [102464/114566]
loss: 0.000456  [108864/114566]
Test Error: 0.000987 

Epoch 9
-------------------------------
loss: 0.000470  [   64/114566]
loss: 0.000513  [ 6464/114566]
loss: 0.000712  [12864/114566]
loss: 0.000668  [19264/114566]
loss: 0.000608  [25664/114566]
loss: 0.000637  [32064/114566]
loss: 0.000876  [38464/114566]
loss: 0.000573  [44864/114566]
loss: 0.000603  [51264/114566]
loss: 0.000432  [57664/114566]
loss: 0.000557  [64064/114566]
loss: 0.000630  [70464/114566]
loss: 0.000527  [76864/114566]
loss: 0.000513  [83264/114566]
loss: 0.000743  [89664/114566]
loss: 0.000487  [96064/114566]
loss: 0.000615  [102464/114566]
loss: 0.000573  [108864/114566]
Test Error: 0.000951 

Epoch 10
-------------------------------
loss: 0.000445  [   64/114566]
loss: 0.000657  [ 6464/114566]
loss: 0.000704  [12864/114566]
loss: 0.000540  [19264/114566]
loss: 0.000652  [25664/114566]
loss: 0.000591  [32064/114566]
loss: 0.000521  [38464/114566]
loss: 0.000474  [44864/114566]
loss: 0.000527  [51264/114566]
loss: 0.000891  [57664/114566]
loss: 0.000526  [64064/114566]
loss: 0.000721  [70464/114566]
loss: 0.000500  [76864/114566]
loss: 0.000490  [83264/114566]
loss: 0.000481  [89664/114566]
loss: 0.000659  [96064/114566]
loss: 0.000534  [102464/114566]
loss: 0.000435  [108864/114566]
Test Error: 0.000919 

Epoch 11
-------------------------------
loss: 0.000522  [   64/114566]
loss: 0.000395  [ 6464/114566]
loss: 0.000560  [12864/114566]
loss: 0.000548  [19264/114566]
loss: 0.000743  [25664/114566]
loss: 0.000451  [32064/114566]
loss: 0.000686  [38464/114566]
loss: 0.000921  [44864/114566]
loss: 0.000604  [51264/114566]
loss: 0.000554  [57664/114566]
loss: 0.000803  [64064/114566]
loss: 0.000634  [70464/114566]
loss: 0.000534  [76864/114566]
loss: 0.001089  [83264/114566]
loss: 0.000629  [89664/114566]
loss: 0.000558  [96064/114566]
loss: 0.000613  [102464/114566]
loss: 0.000563  [108864/114566]
Test Error: 0.000895 

Epoch 12
-------------------------------
loss: 0.000573  [   64/114566]
loss: 0.000575  [ 6464/114566]
loss: 0.000591  [12864/114566]
loss: 0.001142  [19264/114566]
loss: 0.000345  [25664/114566]
loss: 0.000443  [32064/114566]
loss: 0.000480  [38464/114566]
loss: 0.000554  [44864/114566]
loss: 0.000744  [51264/114566]
loss: 0.000424  [57664/114566]
loss: 0.001334  [64064/114566]
loss: 0.000535  [70464/114566]
loss: 0.000666  [76864/114566]
loss: 0.000440  [83264/114566]
loss: 0.000485  [89664/114566]
loss: 0.000642  [96064/114566]
loss: 0.000391  [102464/114566]
loss: 0.000568  [108864/114566]
Test Error: 0.000876 

Epoch 13
-------------------------------
loss: 0.000446  [   64/114566]
loss: 0.000724  [ 6464/114566]
loss: 0.000553  [12864/114566]
loss: 0.000621  [19264/114566]
loss: 0.000490  [25664/114566]
loss: 0.000391  [32064/114566]
loss: 0.000418  [38464/114566]
loss: 0.000409  [44864/114566]
loss: 0.000669  [51264/114566]
loss: 0.000716  [57664/114566]
loss: 0.000590  [64064/114566]
loss: 0.000698  [70464/114566]
loss: 0.000603  [76864/114566]
loss: 0.000413  [83264/114566]
loss: 0.000517  [89664/114566]
loss: 0.000624  [96064/114566]
loss: 0.000891  [102464/114566]
loss: 0.000395  [108864/114566]
Test Error: 0.000861 

Epoch 14
-------------------------------
loss: 0.000470  [   64/114566]
loss: 0.000441  [ 6464/114566]
loss: 0.000866  [12864/114566]
loss: 0.000519  [19264/114566]
loss: 0.000393  [25664/114566]
loss: 0.000521  [32064/114566]
loss: 0.000330  [38464/114566]
loss: 0.001028  [44864/114566]
loss: 0.000504  [51264/114566]
loss: 0.000550  [57664/114566]
loss: 0.000322  [64064/114566]
loss: 0.000657  [70464/114566]
loss: 0.000437  [76864/114566]
loss: 0.000391  [83264/114566]
loss: 0.000520  [89664/114566]
loss: 0.000703  [96064/114566]
loss: 0.000464  [102464/114566]
loss: 0.001306  [108864/114566]
Test Error: 0.000848 

Epoch 15
-------------------------------
loss: 0.000381  [   64/114566]
loss: 0.000768  [ 6464/114566]
loss: 0.000443  [12864/114566]
loss: 0.000787  [19264/114566]
loss: 0.000723  [25664/114566]
loss: 0.000506  [32064/114566]
loss: 0.000655  [38464/114566]
loss: 0.000896  [44864/114566]
loss: 0.000443  [51264/114566]
loss: 0.000474  [57664/114566]
loss: 0.000353  [64064/114566]
loss: 0.000365  [70464/114566]
loss: 0.000993  [76864/114566]
loss: 0.000604  [83264/114566]
loss: 0.000374  [89664/114566]
loss: 0.000377  [96064/114566]
loss: 0.000516  [102464/114566]
loss: 0.000529  [108864/114566]
Test Error: 0.000837 

Epoch 16
-------------------------------
loss: 0.000931  [   64/114566]
loss: 0.000504  [ 6464/114566]
loss: 0.000477  [12864/114566]
loss: 0.000412  [19264/114566]
loss: 0.000752  [25664/114566]
loss: 0.000444  [32064/114566]
loss: 0.000476  [38464/114566]
loss: 0.000630  [44864/114566]
loss: 0.000623  [51264/114566]
loss: 0.000626  [57664/114566]
loss: 0.000452  [64064/114566]
loss: 0.000424  [70464/114566]
loss: 0.000630  [76864/114566]
loss: 0.000465  [83264/114566]
loss: 0.000825  [89664/114566]
loss: 0.000712  [96064/114566]
loss: 0.000473  [102464/114566]
loss: 0.000511  [108864/114566]
Test Error: 0.000827 

Epoch 17
-------------------------------
loss: 0.000494  [   64/114566]
loss: 0.000486  [ 6464/114566]
loss: 0.000501  [12864/114566]
loss: 0.000556  [19264/114566]
loss: 0.000599  [25664/114566]
loss: 0.000577  [32064/114566]
loss: 0.000567  [38464/114566]
loss: 0.000664  [44864/114566]
loss: 0.000570  [51264/114566]
loss: 0.000803  [57664/114566]
loss: 0.000375  [64064/114566]
loss: 0.000578  [70464/114566]
loss: 0.000311  [76864/114566]
loss: 0.000894  [83264/114566]
loss: 0.000433  [89664/114566]
loss: 0.000632  [96064/114566]
loss: 0.000492  [102464/114566]
loss: 0.000496  [108864/114566]
Test Error: 0.000817 

Epoch 18
-------------------------------
loss: 0.000620  [   64/114566]
loss: 0.000509  [ 6464/114566]
loss: 0.000796  [12864/114566]
loss: 0.000472  [19264/114566]
loss: 0.000427  [25664/114566]
loss: 0.000836  [32064/114566]
loss: 0.000384  [38464/114566]
loss: 0.000764  [44864/114566]
loss: 0.000487  [51264/114566]
loss: 0.000501  [57664/114566]
loss: 0.000433  [64064/114566]
loss: 0.000565  [70464/114566]
loss: 0.000389  [76864/114566]
loss: 0.000371  [83264/114566]
loss: 0.000732  [89664/114566]
loss: 0.000733  [96064/114566]
loss: 0.000362  [102464/114566]
loss: 0.000645  [108864/114566]
Test Error: 0.000811 

Epoch 19
-------------------------------
loss: 0.000538  [   64/114566]
loss: 0.000543  [ 6464/114566]
loss: 0.000475  [12864/114566]
loss: 0.000441  [19264/114566]
loss: 0.000404  [25664/114566]
loss: 0.000345  [32064/114566]
loss: 0.000501  [38464/114566]
loss: 0.000479  [44864/114566]
loss: 0.000531  [51264/114566]
loss: 0.000364  [57664/114566]
loss: 0.000890  [64064/114566]
loss: 0.000358  [70464/114566]
loss: 0.000456  [76864/114566]
loss: 0.000471  [83264/114566]
loss: 0.000760  [89664/114566]
loss: 0.001036  [96064/114566]
loss: 0.000599  [102464/114566]
loss: 0.000434  [108864/114566]
Test Error: 0.000806 

Epoch 20
-------------------------------
loss: 0.000495  [   64/114566]
loss: 0.000598  [ 6464/114566]
loss: 0.000462  [12864/114566]
loss: 0.000566  [19264/114566]
loss: 0.000436  [25664/114566]
loss: 0.000493  [32064/114566]
loss: 0.000329  [38464/114566]
loss: 0.000547  [44864/114566]
loss: 0.000263  [51264/114566]
loss: 0.000306  [57664/114566]
loss: 0.000497  [64064/114566]
loss: 0.000508  [70464/114566]
loss: 0.000475  [76864/114566]
loss: 0.000574  [83264/114566]
loss: 0.000685  [89664/114566]
loss: 0.001005  [96064/114566]
loss: 0.000486  [102464/114566]
loss: 0.000919  [108864/114566]
Test Error: 0.000818 

Epoch 21
-------------------------------
loss: 0.000623  [   64/114566]
loss: 0.000339  [ 6464/114566]
loss: 0.000836  [12864/114566]
loss: 0.000607  [19264/114566]
loss: 0.000670  [25664/114566]
loss: 0.000673  [32064/114566]
loss: 0.000394  [38464/114566]
loss: 0.000392  [44864/114566]
loss: 0.000264  [51264/114566]
loss: 0.000372  [57664/114566]
loss: 0.000523  [64064/114566]
loss: 0.001191  [70464/114566]
loss: 0.000395  [76864/114566]
loss: 0.000455  [83264/114566]
loss: 0.000547  [89664/114566]
loss: 0.000388  [96064/114566]
loss: 0.000501  [102464/114566]
loss: 0.000402  [108864/114566]
Test Error: 0.000792 

Epoch 22
-------------------------------
loss: 0.000584  [   64/114566]
loss: 0.000326  [ 6464/114566]
loss: 0.000330  [12864/114566]
loss: 0.000388  [19264/114566]
loss: 0.000690  [25664/114566]
loss: 0.000437  [32064/114566]
loss: 0.000551  [38464/114566]
loss: 0.000550  [44864/114566]
loss: 0.000652  [51264/114566]
loss: 0.000506  [57664/114566]
loss: 0.000509  [64064/114566]
loss: 0.000637  [70464/114566]
loss: 0.000506  [76864/114566]
loss: 0.000523  [83264/114566]
loss: 0.000485  [89664/114566]
loss: 0.001152  [96064/114566]
loss: 0.000455  [102464/114566]
loss: 0.000693  [108864/114566]
Test Error: 0.000788 

Epoch 23
-------------------------------
loss: 0.000364  [   64/114566]
loss: 0.000508  [ 6464/114566]
loss: 0.000591  [12864/114566]
loss: 0.000415  [19264/114566]
loss: 0.000539  [25664/114566]
loss: 0.000574  [32064/114566]
loss: 0.000830  [38464/114566]
loss: 0.000567  [44864/114566]
loss: 0.000381  [51264/114566]
loss: 0.000754  [57664/114566]
loss: 0.000386  [64064/114566]
loss: 0.000659  [70464/114566]
loss: 0.000285  [76864/114566]
loss: 0.000538  [83264/114566]
loss: 0.000606  [89664/114566]
loss: 0.000392  [96064/114566]
loss: 0.000810  [102464/114566]
loss: 0.000314  [108864/114566]
Test Error: 0.000832 

Epoch 24
-------------------------------
loss: 0.000483  [   64/114566]
loss: 0.000448  [ 6464/114566]
loss: 0.000400  [12864/114566]
loss: 0.000696  [19264/114566]
loss: 0.000353  [25664/114566]
loss: 0.000767  [32064/114566]
loss: 0.000406  [38464/114566]
loss: 0.000858  [44864/114566]
loss: 0.000489  [51264/114566]
loss: 0.000640  [57664/114566]
loss: 0.000307  [64064/114566]
loss: 0.000457  [70464/114566]
loss: 0.000425  [76864/114566]
loss: 0.000591  [83264/114566]
loss: 0.000511  [89664/114566]
loss: 0.000379  [96064/114566]
loss: 0.000595  [102464/114566]
loss: 0.000520  [108864/114566]
Test Error: 0.000778 

Epoch 25
-------------------------------
loss: 0.000679  [   64/114566]
loss: 0.000483  [ 6464/114566]
loss: 0.000503  [12864/114566]
loss: 0.000361  [19264/114566]
loss: 0.000471  [25664/114566]
loss: 0.000298  [32064/114566]
loss: 0.000518  [38464/114566]
loss: 0.000379  [44864/114566]
loss: 0.000650  [51264/114566]
loss: 0.000485  [57664/114566]
loss: 0.000494  [64064/114566]
loss: 0.000414  [70464/114566]
loss: 0.000628  [76864/114566]
loss: 0.000602  [83264/114566]
loss: 0.000403  [89664/114566]
loss: 0.000336  [96064/114566]
loss: 0.000582  [102464/114566]
loss: 0.000609  [108864/114566]
Test Error: 0.000777 

Epoch 26
-------------------------------
loss: 0.000440  [   64/114566]
loss: 0.000533  [ 6464/114566]
loss: 0.000472  [12864/114566]
loss: 0.000566  [19264/114566]
loss: 0.000672  [25664/114566]
loss: 0.000375  [32064/114566]
loss: 0.000481  [38464/114566]
loss: 0.000793  [44864/114566]
loss: 0.000593  [51264/114566]
loss: 0.001012  [57664/114566]
loss: 0.000698  [64064/114566]
loss: 0.000561  [70464/114566]
loss: 0.000422  [76864/114566]
loss: 0.001290  [83264/114566]
loss: 0.000403  [89664/114566]
loss: 0.000910  [96064/114566]
loss: 0.000424  [102464/114566]
loss: 0.000603  [108864/114566]
Test Error: 0.000773 

Epoch 27
-------------------------------
loss: 0.000458  [   64/114566]
loss: 0.000394  [ 6464/114566]
loss: 0.000352  [12864/114566]
loss: 0.000586  [19264/114566]
loss: 0.000398  [25664/114566]
loss: 0.000456  [32064/114566]
loss: 0.000339  [38464/114566]
loss: 0.000439  [44864/114566]
loss: 0.000376  [51264/114566]
loss: 0.000604  [57664/114566]
loss: 0.000308  [64064/114566]
loss: 0.000491  [70464/114566]
loss: 0.000338  [76864/114566]
loss: 0.000257  [83264/114566]
loss: 0.000562  [89664/114566]
loss: 0.000394  [96064/114566]
loss: 0.000652  [102464/114566]
loss: 0.000307  [108864/114566]
Test Error: 0.000768 

Epoch 28
-------------------------------
loss: 0.000382  [   64/114566]
loss: 0.000318  [ 6464/114566]
loss: 0.000400  [12864/114566]
loss: 0.000494  [19264/114566]
loss: 0.000536  [25664/114566]
loss: 0.000608  [32064/114566]
loss: 0.000491  [38464/114566]
loss: 0.000459  [44864/114566]
loss: 0.000423  [51264/114566]
loss: 0.000428  [57664/114566]
loss: 0.000273  [64064/114566]
loss: 0.000404  [70464/114566]
loss: 0.000400  [76864/114566]
loss: 0.000355  [83264/114566]
loss: 0.000673  [89664/114566]
loss: 0.000488  [96064/114566]
loss: 0.000321  [102464/114566]
loss: 0.000949  [108864/114566]
Test Error: 0.000764 

Epoch 29
-------------------------------
loss: 0.000401  [   64/114566]
loss: 0.000750  [ 6464/114566]
loss: 0.000975  [12864/114566]
loss: 0.000348  [19264/114566]
loss: 0.000354  [25664/114566]
loss: 0.000371  [32064/114566]
loss: 0.000466  [38464/114566]
loss: 0.000307  [44864/114566]
loss: 0.000412  [51264/114566]
loss: 0.000369  [57664/114566]
loss: 0.000421  [64064/114566]
loss: 0.000349  [70464/114566]
loss: 0.000305  [76864/114566]
loss: 0.000694  [83264/114566]
loss: 0.000408  [89664/114566]
loss: 0.000509  [96064/114566]
loss: 0.000261  [102464/114566]
loss: 0.000739  [108864/114566]
Test Error: 0.000765 

Epoch 30
-------------------------------
loss: 0.000356  [   64/114566]
loss: 0.000364  [ 6464/114566]
loss: 0.000463  [12864/114566]
loss: 0.000609  [19264/114566]
loss: 0.000486  [25664/114566]
loss: 0.000252  [32064/114566]
loss: 0.000401  [38464/114566]
loss: 0.000613  [44864/114566]
loss: 0.000307  [51264/114566]
loss: 0.000385  [57664/114566]
loss: 0.000370  [64064/114566]
loss: 0.000449  [70464/114566]
loss: 0.000367  [76864/114566]
loss: 0.000251  [83264/114566]
loss: 0.000486  [89664/114566]
loss: 0.000366  [96064/114566]
loss: 0.000476  [102464/114566]
loss: 0.000704  [108864/114566]
Test Error: 0.000760 

Epoch 31
-------------------------------
loss: 0.000337  [   64/114566]
loss: 0.000297  [ 6464/114566]
loss: 0.000489  [12864/114566]
loss: 0.000261  [19264/114566]
loss: 0.000227  [25664/114566]
loss: 0.000465  [32064/114566]
loss: 0.000553  [38464/114566]
loss: 0.000528  [44864/114566]
loss: 0.000483  [51264/114566]
loss: 0.000409  [57664/114566]
loss: 0.000410  [64064/114566]
loss: 0.000366  [70464/114566]
loss: 0.000300  [76864/114566]
loss: 0.000716  [83264/114566]
loss: 0.000499  [89664/114566]
loss: 0.000437  [96064/114566]
loss: 0.000607  [102464/114566]
loss: 0.000518  [108864/114566]
Test Error: 0.000755 

Epoch 32
-------------------------------
loss: 0.000544  [   64/114566]
loss: 0.000570  [ 6464/114566]
loss: 0.000600  [12864/114566]
loss: 0.000422  [19264/114566]
loss: 0.000525  [25664/114566]
loss: 0.000520  [32064/114566]
loss: 0.000308  [38464/114566]
loss: 0.000662  [44864/114566]
loss: 0.000517  [51264/114566]
loss: 0.000270  [57664/114566]
loss: 0.000489  [64064/114566]
loss: 0.000335  [70464/114566]
loss: 0.000719  [76864/114566]
loss: 0.000390  [83264/114566]
loss: 0.000459  [89664/114566]
loss: 0.000645  [96064/114566]
loss: 0.000688  [102464/114566]
loss: 0.000408  [108864/114566]
Test Error: 0.000754 

Epoch 33
-------------------------------
loss: 0.000294  [   64/114566]
loss: 0.000383  [ 6464/114566]
loss: 0.000648  [12864/114566]
loss: 0.000572  [19264/114566]
loss: 0.000645  [25664/114566]
loss: 0.000333  [32064/114566]
loss: 0.001005  [38464/114566]
loss: 0.000405  [44864/114566]
loss: 0.000314  [51264/114566]
loss: 0.000418  [57664/114566]
loss: 0.000378  [64064/114566]
loss: 0.000416  [70464/114566]
loss: 0.000346  [76864/114566]
loss: 0.000538  [83264/114566]
loss: 0.000382  [89664/114566]
loss: 0.000527  [96064/114566]
loss: 0.000585  [102464/114566]
loss: 0.000308  [108864/114566]
Test Error: 0.000750 

Epoch 34
-------------------------------
loss: 0.000429  [   64/114566]
loss: 0.000414  [ 6464/114566]
loss: 0.000493  [12864/114566]
loss: 0.000759  [19264/114566]
loss: 0.000335  [25664/114566]
loss: 0.001003  [32064/114566]
loss: 0.000460  [38464/114566]
loss: 0.000321  [44864/114566]
loss: 0.000579  [51264/114566]
loss: 0.000582  [57664/114566]
loss: 0.000460  [64064/114566]
loss: 0.000583  [70464/114566]
loss: 0.000817  [76864/114566]
loss: 0.000543  [83264/114566]
loss: 0.000509  [89664/114566]
loss: 0.000468  [96064/114566]
loss: 0.000564  [102464/114566]
loss: 0.000509  [108864/114566]
Test Error: 0.000752 

Epoch 35
-------------------------------
loss: 0.000382  [   64/114566]
loss: 0.000566  [ 6464/114566]
loss: 0.000633  [12864/114566]
loss: 0.000668  [19264/114566]
loss: 0.000235  [25664/114566]
loss: 0.000823  [32064/114566]
loss: 0.000507  [38464/114566]
loss: 0.000472  [44864/114566]
loss: 0.000471  [51264/114566]
loss: 0.000498  [57664/114566]
loss: 0.000502  [64064/114566]
loss: 0.000333  [70464/114566]
loss: 0.000424  [76864/114566]
loss: 0.000277  [83264/114566]
loss: 0.000540  [89664/114566]
loss: 0.000482  [96064/114566]
loss: 0.000339  [102464/114566]
loss: 0.000290  [108864/114566]
Test Error: 0.000745 

Epoch 36
-------------------------------
loss: 0.000633  [   64/114566]
loss: 0.000370  [ 6464/114566]
loss: 0.000571  [12864/114566]
loss: 0.000268  [19264/114566]
loss: 0.000756  [25664/114566]
loss: 0.000589  [32064/114566]
loss: 0.000531  [38464/114566]
loss: 0.000846  [44864/114566]
loss: 0.000333  [51264/114566]
loss: 0.000366  [57664/114566]
loss: 0.000396  [64064/114566]
loss: 0.000443  [70464/114566]
loss: 0.000925  [76864/114566]
loss: 0.000249  [83264/114566]
loss: 0.000840  [89664/114566]
loss: 0.000329  [96064/114566]
loss: 0.000535  [102464/114566]
loss: 0.000695  [108864/114566]
Test Error: 0.000742 

Epoch 37
-------------------------------
loss: 0.000589  [   64/114566]
loss: 0.000743  [ 6464/114566]
loss: 0.000405  [12864/114566]
loss: 0.000664  [19264/114566]
loss: 0.000823  [25664/114566]
loss: 0.000480  [32064/114566]
loss: 0.000546  [38464/114566]
loss: 0.000335  [44864/114566]
loss: 0.000384  [51264/114566]
loss: 0.000434  [57664/114566]
loss: 0.000424  [64064/114566]
loss: 0.000461  [70464/114566]
loss: 0.000448  [76864/114566]
loss: 0.000374  [83264/114566]
loss: 0.000494  [89664/114566]
loss: 0.000331  [96064/114566]
loss: 0.000519  [102464/114566]
loss: 0.000393  [108864/114566]
Test Error: 0.000742 

Epoch 38
-------------------------------
loss: 0.000698  [   64/114566]
loss: 0.000632  [ 6464/114566]
loss: 0.000755  [12864/114566]
loss: 0.000403  [19264/114566]
loss: 0.000417  [25664/114566]
loss: 0.000412  [32064/114566]
loss: 0.000553  [38464/114566]
loss: 0.001016  [44864/114566]
loss: 0.000674  [51264/114566]
loss: 0.000385  [57664/114566]
loss: 0.000520  [64064/114566]
loss: 0.000491  [70464/114566]
loss: 0.000237  [76864/114566]
loss: 0.000345  [83264/114566]
loss: 0.000534  [89664/114566]
loss: 0.000592  [96064/114566]
loss: 0.000535  [102464/114566]
loss: 0.000462  [108864/114566]
Test Error: 0.000740 

Epoch 39
-------------------------------
loss: 0.000611  [   64/114566]
loss: 0.000541  [ 6464/114566]
loss: 0.000443  [12864/114566]
loss: 0.000421  [19264/114566]
loss: 0.000394  [25664/114566]
loss: 0.000521  [32064/114566]
loss: 0.000480  [38464/114566]
loss: 0.000336  [44864/114566]
loss: 0.000375  [51264/114566]
loss: 0.000313  [57664/114566]
loss: 0.000550  [64064/114566]
loss: 0.000629  [70464/114566]
loss: 0.000709  [76864/114566]
loss: 0.000480  [83264/114566]
loss: 0.000539  [89664/114566]
loss: 0.000348  [96064/114566]
loss: 0.000948  [102464/114566]
loss: 0.000381  [108864/114566]
Test Error: 0.000738 

Epoch 40
-------------------------------
loss: 0.000386  [   64/114566]
loss: 0.000841  [ 6464/114566]
loss: 0.000714  [12864/114566]
loss: 0.000282  [19264/114566]
loss: 0.000511  [25664/114566]
loss: 0.000399  [32064/114566]
loss: 0.000299  [38464/114566]
loss: 0.000741  [44864/114566]
loss: 0.000869  [51264/114566]
loss: 0.000570  [57664/114566]
loss: 0.000480  [64064/114566]
loss: 0.000336  [70464/114566]
loss: 0.000573  [76864/114566]
loss: 0.000496  [83264/114566]
loss: 0.000378  [89664/114566]
loss: 0.000322  [96064/114566]
loss: 0.000506  [102464/114566]
loss: 0.000360  [108864/114566]
Test Error: 0.000737 

Epoch 41
-------------------------------
loss: 0.000421  [   64/114566]
loss: 0.000396  [ 6464/114566]
loss: 0.000404  [12864/114566]
loss: 0.000307  [19264/114566]
loss: 0.000535  [25664/114566]
loss: 0.000683  [32064/114566]
loss: 0.000594  [38464/114566]
loss: 0.000422  [44864/114566]
loss: 0.000685  [51264/114566]
loss: 0.000483  [57664/114566]
loss: 0.000447  [64064/114566]
loss: 0.000572  [70464/114566]
loss: 0.000399  [76864/114566]
loss: 0.000360  [83264/114566]
loss: 0.000571  [89664/114566]
loss: 0.000943  [96064/114566]
loss: 0.000457  [102464/114566]
loss: 0.000475  [108864/114566]
Test Error: 0.000735 

Epoch 42
-------------------------------
loss: 0.000371  [   64/114566]
loss: 0.000360  [ 6464/114566]
loss: 0.000888  [12864/114566]
loss: 0.000512  [19264/114566]
loss: 0.000402  [25664/114566]
loss: 0.000593  [32064/114566]
loss: 0.000515  [38464/114566]
loss: 0.000662  [44864/114566]
loss: 0.000311  [51264/114566]
loss: 0.000364  [57664/114566]
loss: 0.000715  [64064/114566]
loss: 0.000469  [70464/114566]
loss: 0.000408  [76864/114566]
loss: 0.000325  [83264/114566]
loss: 0.000543  [89664/114566]
loss: 0.000953  [96064/114566]
loss: 0.000441  [102464/114566]
loss: 0.000323  [108864/114566]
Test Error: 0.000734 

Epoch 43
-------------------------------
loss: 0.000574  [   64/114566]
loss: 0.000550  [ 6464/114566]
loss: 0.000236  [12864/114566]
loss: 0.000373  [19264/114566]
loss: 0.000462  [25664/114566]
loss: 0.000362  [32064/114566]
loss: 0.000703  [38464/114566]
loss: 0.000477  [44864/114566]
loss: 0.000316  [51264/114566]
loss: 0.000434  [57664/114566]
loss: 0.000517  [64064/114566]
loss: 0.000756  [70464/114566]
loss: 0.000297  [76864/114566]
loss: 0.000518  [83264/114566]
loss: 0.000976  [89664/114566]
loss: 0.000637  [96064/114566]
loss: 0.000545  [102464/114566]
loss: 0.000917  [108864/114566]
Test Error: 0.000736 

Epoch 44
-------------------------------
loss: 0.000879  [   64/114566]
loss: 0.000376  [ 6464/114566]
loss: 0.000328  [12864/114566]
loss: 0.000408  [19264/114566]
loss: 0.000538  [25664/114566]
loss: 0.000999  [32064/114566]
loss: 0.000281  [38464/114566]
loss: 0.000446  [44864/114566]
loss: 0.000463  [51264/114566]
loss: 0.000519  [57664/114566]
loss: 0.000451  [64064/114566]
loss: 0.000576  [70464/114566]
loss: 0.000751  [76864/114566]
loss: 0.000333  [83264/114566]
loss: 0.000460  [89664/114566]
loss: 0.000402  [96064/114566]
loss: 0.000471  [102464/114566]
loss: 0.000599  [108864/114566]
Test Error: 0.000732 

Epoch 45
-------------------------------
loss: 0.000759  [   64/114566]
loss: 0.000526  [ 6464/114566]
loss: 0.000853  [12864/114566]
loss: 0.000429  [19264/114566]
loss: 0.000393  [25664/114566]
loss: 0.000615  [32064/114566]
loss: 0.000500  [38464/114566]
loss: 0.000359  [44864/114566]
loss: 0.000359  [51264/114566]
loss: 0.000361  [57664/114566]
loss: 0.000407  [64064/114566]
loss: 0.000393  [70464/114566]
loss: 0.000692  [76864/114566]
loss: 0.000386  [83264/114566]
loss: 0.000396  [89664/114566]
loss: 0.000552  [96064/114566]
loss: 0.000453  [102464/114566]
loss: 0.000330  [108864/114566]
Test Error: 0.000747 

Epoch 46
-------------------------------
loss: 0.000869  [   64/114566]
loss: 0.000727  [ 6464/114566]
loss: 0.000487  [12864/114566]
loss: 0.000887  [19264/114566]
loss: 0.000481  [25664/114566]
loss: 0.000402  [32064/114566]
loss: 0.000358  [38464/114566]
loss: 0.000515  [44864/114566]
loss: 0.000338  [51264/114566]
loss: 0.000229  [57664/114566]
loss: 0.000394  [64064/114566]
loss: 0.000505  [70464/114566]
loss: 0.000455  [76864/114566]
loss: 0.000342  [83264/114566]
loss: 0.000449  [89664/114566]
loss: 0.000504  [96064/114566]
loss: 0.000623  [102464/114566]
loss: 0.000384  [108864/114566]
Test Error: 0.000728 

Epoch 47
-------------------------------
loss: 0.000483  [   64/114566]
loss: 0.000675  [ 6464/114566]
loss: 0.000413  [12864/114566]
loss: 0.000795  [19264/114566]
loss: 0.000358  [25664/114566]
loss: 0.000390  [32064/114566]
loss: 0.000695  [38464/114566]
loss: 0.000363  [44864/114566]
loss: 0.000341  [51264/114566]
loss: 0.000627  [57664/114566]
loss: 0.000505  [64064/114566]
loss: 0.000354  [70464/114566]
loss: 0.000285  [76864/114566]
loss: 0.000435  [83264/114566]
loss: 0.000547  [89664/114566]
loss: 0.000576  [96064/114566]
loss: 0.000423  [102464/114566]
loss: 0.000693  [108864/114566]
Test Error: 0.000728 

Epoch 48
-------------------------------
loss: 0.000661  [   64/114566]
loss: 0.000407  [ 6464/114566]
loss: 0.000854  [12864/114566]
loss: 0.000609  [19264/114566]
loss: 0.000313  [25664/114566]
loss: 0.000469  [32064/114566]
loss: 0.000285  [38464/114566]
loss: 0.000585  [44864/114566]
loss: 0.000450  [51264/114566]
loss: 0.000639  [57664/114566]
loss: 0.000479  [64064/114566]
loss: 0.000423  [70464/114566]
loss: 0.000475  [76864/114566]
loss: 0.000362  [83264/114566]
loss: 0.000365  [89664/114566]
loss: 0.000677  [96064/114566]
loss: 0.000215  [102464/114566]
loss: 0.000368  [108864/114566]
Test Error: 0.000724 

Epoch 49
-------------------------------
loss: 0.000575  [   64/114566]
loss: 0.000766  [ 6464/114566]
loss: 0.000583  [12864/114566]
loss: 0.000346  [19264/114566]
loss: 0.000414  [25664/114566]
loss: 0.000302  [32064/114566]
loss: 0.000400  [38464/114566]
loss: 0.000428  [44864/114566]
loss: 0.000535  [51264/114566]
loss: 0.000603  [57664/114566]
loss: 0.000416  [64064/114566]
loss: 0.000329  [70464/114566]
loss: 0.000579  [76864/114566]
loss: 0.000272  [83264/114566]
loss: 0.000487  [89664/114566]
loss: 0.000409  [96064/114566]
loss: 0.000625  [102464/114566]
loss: 0.000612  [108864/114566]
Test Error: 0.000726 

Epoch 50
-------------------------------
loss: 0.000376  [   64/114566]
loss: 0.000883  [ 6464/114566]
loss: 0.000305  [12864/114566]
loss: 0.000403  [19264/114566]
loss: 0.000404  [25664/114566]
loss: 0.000551  [32064/114566]
loss: 0.000576  [38464/114566]
loss: 0.001011  [44864/114566]
loss: 0.000322  [51264/114566]
loss: 0.000355  [57664/114566]
loss: 0.000302  [64064/114566]
loss: 0.000513  [70464/114566]
loss: 0.000303  [76864/114566]
loss: 0.000320  [83264/114566]
loss: 0.000437  [89664/114566]
loss: 0.000456  [96064/114566]
loss: 0.000470  [102464/114566]
loss: 0.000315  [108864/114566]
Test Error: 0.000726 

Epoch 51
-------------------------------
loss: 0.000512  [   64/114566]
loss: 0.000412  [ 6464/114566]
loss: 0.000376  [12864/114566]
loss: 0.000420  [19264/114566]
loss: 0.000513  [25664/114566]
loss: 0.000458  [32064/114566]
loss: 0.000275  [38464/114566]
loss: 0.000586  [44864/114566]
loss: 0.000453  [51264/114566]
loss: 0.000518  [57664/114566]
loss: 0.000533  [64064/114566]
loss: 0.000407  [70464/114566]
loss: 0.000476  [76864/114566]
loss: 0.000376  [83264/114566]
loss: 0.000487  [89664/114566]
loss: 0.000448  [96064/114566]
loss: 0.000266  [102464/114566]
loss: 0.000386  [108864/114566]
Test Error: 0.000726 

Epoch 52
-------------------------------
loss: 0.000664  [   64/114566]
loss: 0.000298  [ 6464/114566]
loss: 0.000680  [12864/114566]
loss: 0.000178  [19264/114566]
loss: 0.000568  [25664/114566]
loss: 0.000529  [32064/114566]
loss: 0.000414  [38464/114566]
loss: 0.000518  [44864/114566]
loss: 0.000241  [51264/114566]
loss: 0.000384  [57664/114566]
loss: 0.000324  [64064/114566]
loss: 0.000834  [70464/114566]
loss: 0.000312  [76864/114566]
loss: 0.000279  [83264/114566]
loss: 0.000640  [89664/114566]
loss: 0.000365  [96064/114566]
loss: 0.000381  [102464/114566]
loss: 0.000380  [108864/114566]
Test Error: 0.000721 

Epoch 53
-------------------------------
loss: 0.000777  [   64/114566]
loss: 0.000390  [ 6464/114566]
loss: 0.000352  [12864/114566]
loss: 0.000513  [19264/114566]
loss: 0.000297  [25664/114566]
loss: 0.000425  [32064/114566]
loss: 0.000641  [38464/114566]
loss: 0.000303  [44864/114566]
loss: 0.000355  [51264/114566]
loss: 0.000530  [57664/114566]
loss: 0.000313  [64064/114566]
loss: 0.000327  [70464/114566]
loss: 0.000346  [76864/114566]
loss: 0.000209  [83264/114566]
loss: 0.000522  [89664/114566]
loss: 0.000434  [96064/114566]
loss: 0.000305  [102464/114566]
loss: 0.000540  [108864/114566]
Test Error: 0.000722 

Epoch 54
-------------------------------
loss: 0.001163  [   64/114566]
loss: 0.000398  [ 6464/114566]
loss: 0.000738  [12864/114566]
loss: 0.000294  [19264/114566]
loss: 0.000425  [25664/114566]
loss: 0.000268  [32064/114566]
loss: 0.000307  [38464/114566]
loss: 0.000430  [44864/114566]
loss: 0.001475  [51264/114566]
loss: 0.000539  [57664/114566]
loss: 0.000365  [64064/114566]
loss: 0.000393  [70464/114566]
loss: 0.000393  [76864/114566]
loss: 0.000432  [83264/114566]
loss: 0.000254  [89664/114566]
loss: 0.000282  [96064/114566]
loss: 0.000384  [102464/114566]
loss: 0.000812  [108864/114566]
Test Error: 0.000717 

Epoch 55
-------------------------------
loss: 0.000349  [   64/114566]
loss: 0.000563  [ 6464/114566]
loss: 0.000603  [12864/114566]
loss: 0.000657  [19264/114566]
loss: 0.000344  [25664/114566]
loss: 0.000339  [32064/114566]
loss: 0.000518  [38464/114566]
loss: 0.000281  [44864/114566]
loss: 0.000292  [51264/114566]
loss: 0.000352  [57664/114566]
loss: 0.000352  [64064/114566]
loss: 0.000409  [70464/114566]
loss: 0.000466  [76864/114566]
loss: 0.000480  [83264/114566]
loss: 0.000596  [89664/114566]
loss: 0.000572  [96064/114566]
loss: 0.000472  [102464/114566]
loss: 0.000597  [108864/114566]
Test Error: 0.000726 

Epoch 56
-------------------------------
loss: 0.000486  [   64/114566]
loss: 0.000317  [ 6464/114566]
loss: 0.000310  [12864/114566]
loss: 0.000441  [19264/114566]
loss: 0.000541  [25664/114566]
loss: 0.000540  [32064/114566]
loss: 0.000770  [38464/114566]
loss: 0.000733  [44864/114566]
loss: 0.000514  [51264/114566]
loss: 0.000974  [57664/114566]
loss: 0.000372  [64064/114566]
loss: 0.000930  [70464/114566]
loss: 0.000376  [76864/114566]
loss: 0.000694  [83264/114566]
loss: 0.000690  [89664/114566]
loss: 0.000464  [96064/114566]
loss: 0.000337  [102464/114566]
loss: 0.000891  [108864/114566]
Test Error: 0.000720 

Epoch 57
-------------------------------
loss: 0.000371  [   64/114566]
loss: 0.000396  [ 6464/114566]
loss: 0.000399  [12864/114566]
loss: 0.000373  [19264/114566]
loss: 0.000306  [25664/114566]
loss: 0.000549  [32064/114566]
loss: 0.000538  [38464/114566]
loss: 0.000346  [44864/114566]
loss: 0.000452  [51264/114566]
loss: 0.000328  [57664/114566]
loss: 0.000418  [64064/114566]
loss: 0.000440  [70464/114566]
loss: 0.000347  [76864/114566]
loss: 0.000442  [83264/114566]
loss: 0.000311  [89664/114566]
loss: 0.000747  [96064/114566]
loss: 0.000403  [102464/114566]
loss: 0.000753  [108864/114566]
Test Error: 0.000716 

Epoch 58
-------------------------------
loss: 0.000395  [   64/114566]
loss: 0.000236  [ 6464/114566]
loss: 0.000571  [12864/114566]
loss: 0.000352  [19264/114566]
loss: 0.000467  [25664/114566]
loss: 0.000546  [32064/114566]
loss: 0.000465  [38464/114566]
loss: 0.000393  [44864/114566]
loss: 0.000543  [51264/114566]
loss: 0.000483  [57664/114566]
loss: 0.000624  [64064/114566]
loss: 0.000321  [70464/114566]
loss: 0.000571  [76864/114566]
loss: 0.000426  [83264/114566]
loss: 0.000342  [89664/114566]
loss: 0.000722  [96064/114566]
loss: 0.000432  [102464/114566]
loss: 0.000336  [108864/114566]
Test Error: 0.000717 

Epoch 59
-------------------------------
loss: 0.000561  [   64/114566]
loss: 0.000454  [ 6464/114566]
loss: 0.000353  [12864/114566]
loss: 0.000301  [19264/114566]
loss: 0.000463  [25664/114566]
loss: 0.000361  [32064/114566]
loss: 0.000609  [38464/114566]
loss: 0.001172  [44864/114566]
loss: 0.000324  [51264/114566]
loss: 0.000520  [57664/114566]
loss: 0.000301  [64064/114566]
loss: 0.000476  [70464/114566]
loss: 0.000609  [76864/114566]
loss: 0.000684  [83264/114566]
loss: 0.000551  [89664/114566]
loss: 0.000366  [96064/114566]
loss: 0.000310  [102464/114566]
loss: 0.000537  [108864/114566]
Test Error: 0.000722 

Epoch 60
-------------------------------
loss: 0.000360  [   64/114566]
loss: 0.000510  [ 6464/114566]
loss: 0.000630  [12864/114566]
loss: 0.000960  [19264/114566]
loss: 0.000509  [25664/114566]
loss: 0.000402  [32064/114566]
loss: 0.000631  [38464/114566]
loss: 0.000585  [44864/114566]
loss: 0.000636  [51264/114566]
loss: 0.000387  [57664/114566]
loss: 0.000408  [64064/114566]
loss: 0.000379  [70464/114566]
loss: 0.000840  [76864/114566]
loss: 0.000582  [83264/114566]
loss: 0.000588  [89664/114566]
loss: 0.000653  [96064/114566]
loss: 0.000469  [102464/114566]
loss: 0.000601  [108864/114566]
Test Error: 0.000723 

Epoch 61
-------------------------------
loss: 0.000393  [   64/114566]
loss: 0.000391  [ 6464/114566]
loss: 0.000763  [12864/114566]
loss: 0.000992  [19264/114566]
loss: 0.000434  [25664/114566]
loss: 0.000322  [32064/114566]
loss: 0.000386  [38464/114566]
loss: 0.000326  [44864/114566]
loss: 0.000702  [51264/114566]
loss: 0.000611  [57664/114566]
loss: 0.000501  [64064/114566]
loss: 0.000378  [70464/114566]
loss: 0.000446  [76864/114566]
loss: 0.000528  [83264/114566]
loss: 0.000232  [89664/114566]
loss: 0.000442  [96064/114566]
loss: 0.000888  [102464/114566]
loss: 0.000554  [108864/114566]
Test Error: 0.000717 

Epoch 62
-------------------------------
loss: 0.000649  [   64/114566]
loss: 0.000324  [ 6464/114566]
loss: 0.000314  [12864/114566]
loss: 0.000571  [19264/114566]
loss: 0.000337  [25664/114566]
loss: 0.000572  [32064/114566]
loss: 0.000462  [38464/114566]
loss: 0.000626  [44864/114566]
loss: 0.000328  [51264/114566]
loss: 0.000444  [57664/114566]
loss: 0.000648  [64064/114566]
loss: 0.000439  [70464/114566]
loss: 0.000365  [76864/114566]
loss: 0.000405  [83264/114566]
loss: 0.000707  [89664/114566]
loss: 0.000413  [96064/114566]
loss: 0.000476  [102464/114566]
loss: 0.000491  [108864/114566]
Test Error: 0.000714 

Epoch 63
-------------------------------
loss: 0.000541  [   64/114566]
loss: 0.000353  [ 6464/114566]
loss: 0.000897  [12864/114566]
loss: 0.000346  [19264/114566]
loss: 0.000507  [25664/114566]
loss: 0.000642  [32064/114566]
loss: 0.000741  [38464/114566]
loss: 0.000770  [44864/114566]
loss: 0.000457  [51264/114566]
loss: 0.000412  [57664/114566]
loss: 0.000436  [64064/114566]
loss: 0.000476  [70464/114566]
loss: 0.000534  [76864/114566]
loss: 0.000400  [83264/114566]
loss: 0.000607  [89664/114566]
loss: 0.000432  [96064/114566]
loss: 0.000730  [102464/114566]
loss: 0.000324  [108864/114566]
Test Error: 0.000712 

Epoch 64
-------------------------------
loss: 0.000352  [   64/114566]
loss: 0.000274  [ 6464/114566]
loss: 0.000494  [12864/114566]
loss: 0.000414  [19264/114566]
loss: 0.000637  [25664/114566]
loss: 0.001083  [32064/114566]
loss: 0.000501  [38464/114566]
loss: 0.000470  [44864/114566]
loss: 0.000688  [51264/114566]
loss: 0.000381  [57664/114566]
loss: 0.000534  [64064/114566]
loss: 0.000595  [70464/114566]
loss: 0.000441  [76864/114566]
loss: 0.000436  [83264/114566]
loss: 0.000425  [89664/114566]
loss: 0.000345  [96064/114566]
loss: 0.000319  [102464/114566]
loss: 0.000341  [108864/114566]
Test Error: 0.000714 

Epoch 65
-------------------------------
loss: 0.000430  [   64/114566]
loss: 0.000747  [ 6464/114566]
loss: 0.001052  [12864/114566]
loss: 0.000395  [19264/114566]
loss: 0.000430  [25664/114566]
loss: 0.000373  [32064/114566]
loss: 0.001217  [38464/114566]
loss: 0.000418  [44864/114566]
loss: 0.000606  [51264/114566]
loss: 0.000829  [57664/114566]
loss: 0.000395  [64064/114566]
loss: 0.000331  [70464/114566]
loss: 0.000501  [76864/114566]
loss: 0.000513  [83264/114566]
loss: 0.000417  [89664/114566]
loss: 0.000627  [96064/114566]
loss: 0.000588  [102464/114566]
loss: 0.000648  [108864/114566]
Test Error: 0.000713 

Epoch 66
-------------------------------
loss: 0.000412  [   64/114566]
loss: 0.000349  [ 6464/114566]
loss: 0.000550  [12864/114566]
loss: 0.000408  [19264/114566]
loss: 0.000965  [25664/114566]
loss: 0.000239  [32064/114566]
loss: 0.000464  [38464/114566]
loss: 0.000599  [44864/114566]
loss: 0.000292  [51264/114566]
loss: 0.000349  [57664/114566]
loss: 0.000738  [64064/114566]
loss: 0.000481  [70464/114566]
loss: 0.000422  [76864/114566]
loss: 0.000754  [83264/114566]
loss: 0.000626  [89664/114566]
loss: 0.000534  [96064/114566]
loss: 0.000433  [102464/114566]
loss: 0.000377  [108864/114566]
Test Error: 0.000714 

Epoch 67
-------------------------------
loss: 0.000522  [   64/114566]
loss: 0.000486  [ 6464/114566]
loss: 0.000575  [12864/114566]
loss: 0.000377  [19264/114566]
loss: 0.000530  [25664/114566]
loss: 0.000649  [32064/114566]
loss: 0.000428  [38464/114566]
loss: 0.000287  [44864/114566]
loss: 0.000513  [51264/114566]
loss: 0.000460  [57664/114566]
loss: 0.000298  [64064/114566]
loss: 0.000356  [70464/114566]
loss: 0.000315  [76864/114566]
loss: 0.000295  [83264/114566]
loss: 0.000400  [89664/114566]
loss: 0.000504  [96064/114566]
loss: 0.000528  [102464/114566]
loss: 0.000348  [108864/114566]
Test Error: 0.000713 

Epoch 68
-------------------------------
loss: 0.000449  [   64/114566]
loss: 0.000392  [ 6464/114566]
loss: 0.000622  [12864/114566]
loss: 0.000458  [19264/114566]
loss: 0.000575  [25664/114566]
loss: 0.000385  [32064/114566]
loss: 0.000601  [38464/114566]
loss: 0.000510  [44864/114566]
loss: 0.000596  [51264/114566]
loss: 0.000360  [57664/114566]
loss: 0.000346  [64064/114566]
loss: 0.000417  [70464/114566]
loss: 0.000614  [76864/114566]
loss: 0.000681  [83264/114566]
loss: 0.000283  [89664/114566]
loss: 0.000571  [96064/114566]
loss: 0.000361  [102464/114566]
loss: 0.000341  [108864/114566]
Test Error: 0.000729 

Epoch 69
-------------------------------
loss: 0.000721  [   64/114566]
loss: 0.000620  [ 6464/114566]
loss: 0.000319  [12864/114566]
loss: 0.000462  [19264/114566]
loss: 0.000310  [25664/114566]
loss: 0.000991  [32064/114566]
loss: 0.000497  [38464/114566]
loss: 0.000393  [44864/114566]
loss: 0.000530  [51264/114566]
loss: 0.000422  [57664/114566]
loss: 0.000332  [64064/114566]
loss: 0.001028  [70464/114566]
loss: 0.000500  [76864/114566]
loss: 0.000368  [83264/114566]
loss: 0.000575  [89664/114566]
loss: 0.000431  [96064/114566]
loss: 0.000391  [102464/114566]
loss: 0.000420  [108864/114566]
Test Error: 0.000739 

Epoch 70
-------------------------------
loss: 0.000680  [   64/114566]
loss: 0.000435  [ 6464/114566]
loss: 0.000505  [12864/114566]
loss: 0.000335  [19264/114566]
loss: 0.000281  [25664/114566]
loss: 0.000391  [32064/114566]
loss: 0.000310  [38464/114566]
loss: 0.000433  [44864/114566]
loss: 0.000406  [51264/114566]
loss: 0.001098  [57664/114566]
loss: 0.000524  [64064/114566]
loss: 0.000312  [70464/114566]
loss: 0.000586  [76864/114566]
loss: 0.000589  [83264/114566]
loss: 0.000446  [89664/114566]
loss: 0.000426  [96064/114566]
loss: 0.000403  [102464/114566]
loss: 0.000267  [108864/114566]
Test Error: 0.000708 

Epoch 71
-------------------------------
loss: 0.000379  [   64/114566]
loss: 0.000393  [ 6464/114566]
loss: 0.000374  [12864/114566]
loss: 0.000474  [19264/114566]
loss: 0.000378  [25664/114566]
loss: 0.000644  [32064/114566]
loss: 0.000354  [38464/114566]
loss: 0.000374  [44864/114566]
loss: 0.000818  [51264/114566]
loss: 0.000431  [57664/114566]
loss: 0.000407  [64064/114566]
loss: 0.000501  [70464/114566]
loss: 0.000956  [76864/114566]
loss: 0.000358  [83264/114566]
loss: 0.000326  [89664/114566]
loss: 0.000588  [96064/114566]
loss: 0.000479  [102464/114566]
loss: 0.000632  [108864/114566]
Test Error: 0.000708 

Epoch 72
-------------------------------
loss: 0.000340  [   64/114566]
loss: 0.000584  [ 6464/114566]
loss: 0.000672  [12864/114566]
loss: 0.000535  [19264/114566]
loss: 0.000483  [25664/114566]
loss: 0.000319  [32064/114566]
loss: 0.000579  [38464/114566]
loss: 0.000503  [44864/114566]
loss: 0.000390  [51264/114566]
loss: 0.000366  [57664/114566]
loss: 0.000480  [64064/114566]
loss: 0.000692  [70464/114566]
loss: 0.000347  [76864/114566]
loss: 0.000394  [83264/114566]
loss: 0.000516  [89664/114566]
loss: 0.000334  [96064/114566]
loss: 0.000534  [102464/114566]
loss: 0.000927  [108864/114566]
Test Error: 0.000708 

Epoch 73
-------------------------------
loss: 0.000434  [   64/114566]
loss: 0.000407  [ 6464/114566]
loss: 0.000527  [12864/114566]
loss: 0.000300  [19264/114566]
loss: 0.000386  [25664/114566]
loss: 0.000365  [32064/114566]
loss: 0.000605  [38464/114566]
loss: 0.000750  [44864/114566]
loss: 0.000338  [51264/114566]
loss: 0.000478  [57664/114566]
loss: 0.000382  [64064/114566]
loss: 0.000340  [70464/114566]
loss: 0.000637  [76864/114566]
loss: 0.000353  [83264/114566]
loss: 0.000400  [89664/114566]
loss: 0.000398  [96064/114566]
loss: 0.001321  [102464/114566]
loss: 0.000321  [108864/114566]
Test Error: 0.000707 

Epoch 74
-------------------------------
loss: 0.000366  [   64/114566]
loss: 0.000493  [ 6464/114566]
loss: 0.000342  [12864/114566]
loss: 0.000468  [19264/114566]
loss: 0.000504  [25664/114566]
loss: 0.000354  [32064/114566]
loss: 0.000622  [38464/114566]
loss: 0.000295  [44864/114566]
loss: 0.000651  [51264/114566]
loss: 0.000555  [57664/114566]
loss: 0.000414  [64064/114566]
loss: 0.000343  [70464/114566]
loss: 0.000647  [76864/114566]
loss: 0.000447  [83264/114566]
loss: 0.000371  [89664/114566]
loss: 0.000623  [96064/114566]
loss: 0.000394  [102464/114566]
loss: 0.000473  [108864/114566]
Test Error: 0.000708 

Epoch 75
-------------------------------
loss: 0.000576  [   64/114566]
loss: 0.000388  [ 6464/114566]
loss: 0.000603  [12864/114566]
loss: 0.000573  [19264/114566]
loss: 0.000284  [25664/114566]
loss: 0.000427  [32064/114566]
loss: 0.000396  [38464/114566]
loss: 0.000581  [44864/114566]
loss: 0.000356  [51264/114566]
loss: 0.000572  [57664/114566]
loss: 0.000464  [64064/114566]
loss: 0.000950  [70464/114566]
loss: 0.000413  [76864/114566]
loss: 0.000380  [83264/114566]
loss: 0.000507  [89664/114566]
loss: 0.001173  [96064/114566]
loss: 0.000243  [102464/114566]
loss: 0.000513  [108864/114566]
Test Error: 0.000709 

Epoch 76
-------------------------------
loss: 0.000471  [   64/114566]
loss: 0.000627  [ 6464/114566]
loss: 0.000380  [12864/114566]
loss: 0.000498  [19264/114566]
loss: 0.000471  [25664/114566]
loss: 0.000386  [32064/114566]
loss: 0.000284  [38464/114566]
loss: 0.000390  [44864/114566]
loss: 0.000427  [51264/114566]
loss: 0.000497  [57664/114566]
loss: 0.000289  [64064/114566]
loss: 0.000631  [70464/114566]
loss: 0.000351  [76864/114566]
loss: 0.000919  [83264/114566]
loss: 0.000364  [89664/114566]
loss: 0.000414  [96064/114566]
loss: 0.000327  [102464/114566]
loss: 0.000423  [108864/114566]
Test Error: 0.000707 

Epoch 77
-------------------------------
loss: 0.000445  [   64/114566]
loss: 0.000538  [ 6464/114566]
loss: 0.000312  [12864/114566]
loss: 0.000505  [19264/114566]
loss: 0.000366  [25664/114566]
loss: 0.000731  [32064/114566]
loss: 0.000337  [38464/114566]
loss: 0.000354  [44864/114566]
loss: 0.000641  [51264/114566]
loss: 0.001451  [57664/114566]
loss: 0.000627  [64064/114566]
loss: 0.000585  [70464/114566]
loss: 0.000510  [76864/114566]
loss: 0.000358  [83264/114566]
loss: 0.000282  [89664/114566]
loss: 0.000704  [96064/114566]
loss: 0.000489  [102464/114566]
loss: 0.000351  [108864/114566]
Test Error: 0.000707 

Epoch 78
-------------------------------
loss: 0.000413  [   64/114566]
loss: 0.000289  [ 6464/114566]
loss: 0.000306  [12864/114566]
loss: 0.000524  [19264/114566]
loss: 0.000544  [25664/114566]
loss: 0.000229  [32064/114566]
loss: 0.000354  [38464/114566]
loss: 0.000301  [44864/114566]
loss: 0.000515  [51264/114566]
loss: 0.000503  [57664/114566]
loss: 0.000385  [64064/114566]
loss: 0.000816  [70464/114566]
loss: 0.000889  [76864/114566]
loss: 0.000497  [83264/114566]
loss: 0.000347  [89664/114566]
loss: 0.000342  [96064/114566]
loss: 0.000489  [102464/114566]
loss: 0.000583  [108864/114566]
Test Error: 0.000705 

Epoch 79
-------------------------------
loss: 0.000241  [   64/114566]
loss: 0.000278  [ 6464/114566]
loss: 0.000336  [12864/114566]
loss: 0.000439  [19264/114566]
loss: 0.000467  [25664/114566]
loss: 0.000808  [32064/114566]
loss: 0.000507  [38464/114566]
loss: 0.000721  [44864/114566]
loss: 0.000276  [51264/114566]
loss: 0.000449  [57664/114566]
loss: 0.000406  [64064/114566]
loss: 0.000548  [70464/114566]
loss: 0.000576  [76864/114566]
loss: 0.000348  [83264/114566]
loss: 0.000471  [89664/114566]
loss: 0.000406  [96064/114566]
loss: 0.000505  [102464/114566]
loss: 0.000536  [108864/114566]
Test Error: 0.000707 

Epoch 80
-------------------------------
loss: 0.000688  [   64/114566]
loss: 0.000435  [ 6464/114566]
loss: 0.000698  [12864/114566]
loss: 0.000477  [19264/114566]
loss: 0.000473  [25664/114566]
loss: 0.000493  [32064/114566]
loss: 0.000755  [38464/114566]
loss: 0.000572  [44864/114566]
loss: 0.000550  [51264/114566]
loss: 0.000430  [57664/114566]
loss: 0.000422  [64064/114566]
loss: 0.000366  [70464/114566]
loss: 0.000799  [76864/114566]
loss: 0.000466  [83264/114566]
loss: 0.000337  [89664/114566]
loss: 0.000323  [96064/114566]
loss: 0.000330  [102464/114566]
loss: 0.000368  [108864/114566]
Test Error: 0.000707 

            Returns_1  Returns_2  Returns_3  Returns_4  Returns_5
Date                                                             
2025-03-25   0.005187  -0.002604   0.000167  -0.002830  -0.001425
2025-03-26   0.009807   0.004007   0.002228   0.006086   0.004259
2025-03-27   0.006680  -0.002246   0.001141  -0.001390  -0.000214
2025-03-28   0.026039   0.004310  -0.001756   0.002449   0.000501
2025-03-31  -0.033380   0.007133  -0.002300   0.003982   0.001074
2025-04-01  -0.030629  -0.001245  -0.003210  -0.005056  -0.004152
2025-04-02  -0.025172   0.001325  -0.001086  -0.000467  -0.000738
2025-04-03   0.007186  -0.004315   0.004262  -0.001158   0.001346
2025-04-04   0.029930   0.006546  -0.004876   0.001841  -0.001254
2025-04-07   0.019846  -0.000680   0.001721   0.000664   0.001145

"""